{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Setup**\n",
        "\n",
        "This cell installs the timm (PyTorch Image Models) library and imports all necessary dependencies."
      ],
      "metadata": {
        "id": "Z_5r4IL_USK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3bO1hdR1XGt"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "!pip install timm\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import argparse\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch import autograd\n",
        "import copy\n",
        "import pandas as pd\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantization Modules**\n",
        "\n",
        "Defines the Quantizer_weight (per-channel) and Quantizer_activation (per-image) classes."
      ],
      "metadata": {
        "id": "tOAkStdwVIuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Round(autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, inputs):\n",
        "        return torch.floor(inputs + 0.5)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        return grads\n",
        "\n",
        "class Quantizer_weight(nn.Module):\n",
        "# quantizises the activations per image rather than per batch and weights per channel rather than layer\n",
        "    def __init__(self, bits_precision):\n",
        "        super().__init__()\n",
        "        self.bits_precision = bits_precision\n",
        "\n",
        "    def quantize(self,inputs,scale):\n",
        "        outputs = inputs*(1/scale)\n",
        "        return outputs\n",
        "\n",
        "    def round(self, inputs):\n",
        "        outputs = Round.apply(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def clamp(self,inputs):\n",
        "        outputs = torch.clamp(inputs,-2**(self.bits_precision-1) + 1,2**(self.bits_precision-1) - 1)\n",
        "        return outputs\n",
        "\n",
        "    def dequantize(self, inputs,scale):\n",
        "        outputs = scale*(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if len(inputs.shape) == 4:\n",
        "          tup = (1,2,3)\n",
        "        else:\n",
        "          tup = (1,)\n",
        "        inp_min = torch.clone(inputs)\n",
        "        inp_max = torch.clone(inputs)\n",
        "        for dim in tup:\n",
        "          inp_min = torch.min(inp_min,dim = dim,keepdim = True)[0]\n",
        "          inp_max = torch.max(inp_max,dim = dim,keepdim = True)[0]\n",
        "        scale = (inp_max - inp_min + 1e-15)/(2**(self.bits_precision) - 2)\n",
        "        outputs = self.quantize(inputs,scale)\n",
        "        outputs = self.round(outputs)\n",
        "        outputs = self.clamp(outputs)\n",
        "        outputs = self.dequantize(outputs,scale)\n",
        "        return outputs,scale\n",
        "\n",
        "class Quantizer_activation(nn.Module):\n",
        "# quantizises the activations per image rather than per batch and weights per channel rather than layer\n",
        "    def __init__(self, bits_precision):\n",
        "        super().__init__()\n",
        "        self.bits_precision = bits_precision\n",
        "\n",
        "    def quantize(self,inputs,scale):\n",
        "        outputs = inputs*(1/scale)\n",
        "        return outputs\n",
        "\n",
        "    def round(self, inputs):\n",
        "        outputs = Round.apply(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def clamp(self,inputs):\n",
        "        outputs = torch.clamp(inputs,-2**(self.bits_precision-1),2**(self.bits_precision) - 1)\n",
        "        return outputs\n",
        "\n",
        "    def dequantize(self, inputs,scale):\n",
        "        outputs = scale*(inputs)\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if len(inputs.shape) == 4:\n",
        "          tup = (1,2,3)\n",
        "        elif len(inputs.shape) ==3:\n",
        "          tup = (1,2)\n",
        "        else:\n",
        "          tup = (1,)\n",
        "        inp_min = torch.clone(inputs)\n",
        "        inp_max = torch.clone(inputs)\n",
        "        for dim in tup:\n",
        "          inp_min = torch.min(inp_min,dim = dim,keepdim = True)[0]\n",
        "          inp_max = torch.max(inp_max,dim = dim,keepdim = True)[0]\n",
        "        scale = (inp_max - inp_min + 1e-15)/(2**(self.bits_precision) - 1)\n",
        "        outputs = self.quantize(inputs,scale)\n",
        "        outputs = self.round(outputs)\n",
        "        outputs = self.clamp(outputs)\n",
        "        outputs = self.dequantize(outputs,scale)\n",
        "        return outputs,scale"
      ],
      "metadata": {
        "id": "RLlopOAc1eop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fault Injection Infrastructure: LUTs and Fallbacks**\n",
        "\n",
        "This block implements the core logic for efficient fault simulation and mitigation:\n",
        "\n",
        "`build_small_fault_lut`: Precomputes a Look-Up Table (LUT) on the GPU. For every possible n-bit weight and every \"small\" fault pattern (defined by max_forced_bits), it calculates the closest valid integer value.\n",
        "\n",
        "`make_forced_mask_val`: A helper that uses matrix multiplication to convert bitwise fault maps (shape bitwidth x N) into integer masks for fast LUT indexing.\n",
        "\n",
        "`fallback_many_weights`: A robust fallback mechanism in case the LUT does not contain a case encountered."
      ],
      "metadata": {
        "id": "QlPVvUzYYm86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from itertools import combinations, product\n",
        "\n",
        "##############################################################################\n",
        "# 1) Build a global LUT on GPU\n",
        "##############################################################################\n",
        "def enumerate_small_fault_patterns(bitwidth=8, max_forced_bits=2):\n",
        "    \"\"\"\n",
        "    Return a list of (pattern_code, forced_mask, forced_val) for all patterns\n",
        "    that have <= max_forced_bits forced bits in an 8-bit number.\n",
        "    \"\"\"\n",
        "    patterns = []\n",
        "    pcode = 0\n",
        "\n",
        "    # For k=0..max_forced_bits\n",
        "    for k in range(max_forced_bits+1):\n",
        "        if k == 0:\n",
        "            # No forced bits => (mask=0, val=0)\n",
        "            patterns.append((pcode, 0, 0))\n",
        "            pcode += 1\n",
        "        else:\n",
        "            # pick which k bits to force\n",
        "            for bits_chosen in combinations(range(bitwidth), k):\n",
        "                # each forced bit can be forced to 0 or 1\n",
        "                for forced_vals in product([0,1], repeat=k):\n",
        "                    mask = 0\n",
        "                    val  = 0\n",
        "                    for bit_pos, f_val in zip(bits_chosen, forced_vals):\n",
        "                        mask |= (1 << bit_pos)\n",
        "                        if f_val == 1:\n",
        "                            val |= (1 << bit_pos)\n",
        "                    patterns.append((pcode, mask, val))\n",
        "                    pcode += 1\n",
        "\n",
        "    return patterns\n",
        "\n",
        "def build_small_fault_lut(bitwidth=8, max_forced_bits=2, device='cuda'):\n",
        "    \"\"\"\n",
        "    Build:\n",
        "      global_lut: (256, num_patterns) => best readout (int16 in [-128..127])\n",
        "      pattern_id_map: shape (65536,) => maps (mask<<8 | val) -> pattern_idx or -1\n",
        "    On the specified device (e.g. 'cuda').\n",
        "    \"\"\"\n",
        "    all_patterns = enumerate_small_fault_patterns(bitwidth, max_forced_bits)\n",
        "    num_patterns = len(all_patterns)\n",
        "\n",
        "    # 1) Build the LUT array on GPU\n",
        "    global_lut = torch.empty((256, num_patterns), dtype=torch.int16, device=device)\n",
        "\n",
        "    # Precompute all 256 combos of 8-bit two's complement => [-128..127]\n",
        "    all_u8 = torch.arange(256, dtype=torch.uint8, device=device)\n",
        "    all_s8 = all_u8.to(torch.int16).clone()\n",
        "    mask_neg = (all_s8 >= 128)\n",
        "    all_s8[mask_neg] -= 256  # now in [-128..127]\n",
        "\n",
        "    xs = torch.arange(-128, 128, dtype=torch.int16, device=device)\n",
        "    xs_offset = xs + 128\n",
        "\n",
        "    for i, xval in enumerate(xs):\n",
        "        # Distance from xval\n",
        "        dist_all = (all_s8 - xval).abs()\n",
        "\n",
        "        for pat_idx, (pcode, fm, fv) in enumerate(all_patterns):\n",
        "            # valid combos => (all_u8 & fm) == fv\n",
        "            masked_bits = (all_u8 & fm)\n",
        "            forced_ok = (masked_bits == fv)\n",
        "            valid_indices = torch.where(forced_ok)[0]\n",
        "            if len(valid_indices)==0:\n",
        "                # No valid combos, pick xval itself\n",
        "                best_val = xval.item()\n",
        "            else:\n",
        "                subdist = dist_all[valid_indices]\n",
        "                best_subidx = torch.argmin(subdist)\n",
        "                global_idx = valid_indices[best_subidx]\n",
        "                best_val = all_s8[global_idx].item()\n",
        "            global_lut[xs_offset[i].item(), pat_idx] = best_val\n",
        "\n",
        "    # 2) pattern_id_map => shape(65536,), store on GPU\n",
        "    pattern_id_map = -1 * torch.ones((1<<(2*bitwidth),), dtype=torch.int16, device=device)\n",
        "    for pat_idx, (pcode, fm, fv) in enumerate(all_patterns):\n",
        "        combo_id = (fm << 8) | fv\n",
        "        pattern_id_map[combo_id] = pat_idx\n",
        "\n",
        "    return global_lut, pattern_id_map, all_patterns\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 2) Helper to build forced_mask/val using float matmul on GPU\n",
        "##############################################################################\n",
        "def make_forced_mask_val(faults_2d: torch.Tensor):\n",
        "    \"\"\"\n",
        "    faults_2d: shape (bitwidth, N) in {0, +1, -1}, on GPU, typically float or int.\n",
        "    Returns (forced_mask, forced_val, forced_count) each shape(N,) in int16.\n",
        "    We do an internal float matmul, then cast to int16 at the end.\n",
        "    \"\"\"\n",
        "    device = faults_2d.device\n",
        "    bitwidth, N = faults_2d.shape\n",
        "\n",
        "    # powers_of_two in [1,2,4,...]\n",
        "    powers_of_two = (1 << torch.arange(bitwidth, device=device, dtype=torch.int64)).to(torch.float32)\n",
        "\n",
        "    # Convert faults_2d to float32 if not already\n",
        "    f_2d_float = faults_2d.to(torch.float32)\n",
        "\n",
        "    forced_mask_bool = (f_2d_float != 0.0).to(torch.float32)   # (bitwidth, N)\n",
        "    forced_mask_32 = forced_mask_bool.T @ powers_of_two        # (N,)\n",
        "\n",
        "    forced_val_bool = (f_2d_float == 1.0).to(torch.float32)\n",
        "    forced_val_32   = forced_val_bool.T @ powers_of_two        # (N,)\n",
        "\n",
        "    forced_count_32 = forced_mask_bool.sum(dim=0)              # (N,)\n",
        "\n",
        "    forced_mask  = forced_mask_32.to(torch.int16)\n",
        "    forced_val   = forced_val_32.to(torch.int16)\n",
        "    forced_count = forced_count_32.to(torch.int16)\n",
        "\n",
        "    return forced_mask, forced_val, forced_count\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# 3) Fallback routines, all GPU-compatible, returning INT16\n",
        "##############################################################################\n",
        "def fallback_many_weights(\n",
        "    targets_s8: torch.Tensor,  # shape(M,) in [-128..127], int16\n",
        "    faults_2d:  torch.Tensor,  # shape(bitwidth,M) in {0,+1,-1}, float/int\n",
        "    bitwidth=8\n",
        ")->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Vector fallback: enumerates all 2^bitwidth combos for each item.\n",
        "    Returns shape(M,) => best read for each item in INT16 ([-128..127]).\n",
        "    \"\"\"\n",
        "    device = targets_s8.device\n",
        "    M = targets_s8.shape[0]\n",
        "\n",
        "    # all 2^bitwidth combos in [-128..127]\n",
        "    all_u8 = torch.arange(1<<bitwidth, dtype=torch.uint8, device=device)\n",
        "    all_s8 = all_u8.to(torch.int16).clone()\n",
        "    mask_neg = (all_s8 >= (1<<(bitwidth-1)))\n",
        "    all_s8[mask_neg] -= (1<<bitwidth)\n",
        "\n",
        "    # Expand combos bitwise\n",
        "    bpos = torch.arange(bitwidth, device=device).view(1, -1)\n",
        "    combos_8 = ((all_u8.unsqueeze(-1) >> bpos) & 1).to(torch.int8)  # shape(256, bitwidth)\n",
        "\n",
        "    # We interpret faults_2d\n",
        "    # forced_1 => (f_t==1)\n",
        "    # forced_0 => (f_t==-1)\n",
        "    f_t = faults_2d.to(torch.int8).T  # shape(M, bitwidth)\n",
        "    forced_1 = (f_t == 1)\n",
        "    forced_0 = (f_t == -1)\n",
        "\n",
        "    # combos_expanded: shape(M,256,bitwidth)\n",
        "    combos_expanded = combos_8.unsqueeze(0).expand(M, -1, -1)\n",
        "    forced_1_ok = (~forced_1.unsqueeze(1)) | (combos_expanded == 1)\n",
        "    forced_0_ok = (~forced_0.unsqueeze(1)) | (combos_expanded == 0)\n",
        "    forced_ok   = torch.logical_and(forced_1_ok, forced_0_ok).all(dim=-1)  # shape(M,256)\n",
        "\n",
        "    # Distance to each combo\n",
        "    # Convert targets_s8 => int16 => shape(M,)\n",
        "    # We'll do an int32 distance to avoid any overflow.\n",
        "    t_s32 = targets_s8.to(torch.int32)\n",
        "    all_s8_s32 = all_s8.to(torch.int32)  # shape(256,)\n",
        "    dist = (all_s8_s32.view(1, -1) - t_s32.view(-1, 1)).abs()  # (M,256) in int32\n",
        "\n",
        "    # Mark invalid combos with very large distance\n",
        "    dist[~forced_ok] = 999999\n",
        "\n",
        "    best_idx = dist.argmin(dim=1)  # (M,)\n",
        "    best_vals = all_s8[best_idx]   # shape(M,) in int16\n",
        "    return best_vals"
      ],
      "metadata": {
        "id": "YN-xM5rijfdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_lut, pattern_id_map,_ = build_small_fault_lut(bitwidth=8, max_forced_bits=2)"
      ],
      "metadata": {
        "id": "Ht6xII0SgG7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Quantized Linear Layer with Fault Injection**\n",
        "\n",
        "This class extends nn.Linear to support weight and activation quantization and stuck-at fault (SAF) simulation.\n",
        "\n",
        "`inject_faults`: The central method for simulating SAFs. It injects \"stuck-at\" faults into the weights and applies advanced mitigation strategies:\n",
        "\n",
        "`none`: Baseline in this work - Closest Value Mapping (CVM).\n",
        "\n",
        "`flip`: Sign-Flip technique.\n",
        "\n",
        "`flip-bitwise`: Bit-Flip technique.\n",
        "\n",
        "`forward`: Executes the quantized matrix multiplication, handling input/weight scaling factors to restore the full-precision output range."
      ],
      "metadata": {
        "id": "LQootDpeYwE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class custom_QuantizedLinear_new_baseline(nn.Linear):\n",
        "    def __init__(self, *args,**kwargs):\n",
        "        super(custom_QuantizedLinear_new_baseline, self).__init__(*args,**kwargs)\n",
        "        self.weight_precision = 8\n",
        "        self.input_precision  = 8\n",
        "        self.array_dimension  = 64\n",
        "        self.inference_done   = False\n",
        "        self.weight_scale     = 0\n",
        "        self.max_forced_bits = 2\n",
        "        self.global_lut       = global_lut.to('cuda')            # shape(256, numPatterns)\n",
        "        self.pattern_id_map   = pattern_id_map.to('cuda')        # shape(65536,)\n",
        "        # placeholders for your actual quantizers\n",
        "        self.weight_quantizer = Quantizer_weight(bits_precision=self.weight_precision)\n",
        "        self.input_quantizer  = Quantizer_activation(bits_precision=self.input_precision)\n",
        "\n",
        "    def to_inference(self):\n",
        "        # quantize and store final\n",
        "        weight, weight_scale = self.weight_quantizer(self.weight)\n",
        "        self.weight_scale = weight_scale\n",
        "        self.weight.data = weight\n",
        "        self.inference_done = True\n",
        "\n",
        "    def inject_faults(self,\n",
        "                      fault_rate: float,\n",
        "                      fault_mitigation: str='none',\n",
        "                      row_block_size: int=64):\n",
        "        \"\"\"\n",
        "        All modes use the LUT for recognized items (<= max_forced_bits).\n",
        "        For 'bad' items, do a fully vector fallback.\n",
        "\n",
        "        The \"flip-bitwise\" path enumerates the 256 possible bit flips in **one** pass\n",
        "        per row-block (no sub-batching), stored in int-safe arrays.\n",
        "        \"\"\"\n",
        "        device = self.weight.device\n",
        "        bitwidth = self.weight_precision\n",
        "\n",
        "        # 1) Convert original weights to int16 domain\n",
        "        w_float   = self.weight / self.weight_scale\n",
        "        w_rounded = Round.apply(w_float).clamp(-128,127)\n",
        "        weight_2d = w_rounded.transpose(0,1).contiguous()  # shape(in_feats, out_feats)\n",
        "        in_feats, out_feats = weight_2d.shape\n",
        "\n",
        "        # 2) Build random stuck bits => shape(bitwidth, N)\n",
        "        N = in_feats*out_feats\n",
        "        mat = (fault_rate/100.0)*torch.ones(bitwidth, N, device=device)\n",
        "        f_matrix= torch.bernoulli(mat)               # in {0,1} float\n",
        "        sign_mat= (2*torch.bernoulli(0.5*torch.ones_like(mat))-1)  # in {-1,+1}\n",
        "        faults_2d= f_matrix*sign_mat  # in {0, -1, +1}, shape(bitwidth,N)\n",
        "\n",
        "        # We'll store final result in new_weight_2d => shape(in_feats, out_feats)\n",
        "        new_weight_2d = torch.empty_like(weight_2d, dtype=torch.int16, device=device)\n",
        "\n",
        "        if fault_mitigation == 'none':\n",
        "            # Single-pass vector approach: LUT for recognized, fallback for others\n",
        "            flat_w = weight_2d.view(-1).to(torch.int16)\n",
        "            fm, fv, fc = make_forced_mask_val(faults_2d)\n",
        "            combo_id   = ((fm << 8) | fv).to(torch.int64)\n",
        "            pat_idx    = self.pattern_id_map[combo_id].to(torch.long)\n",
        "\n",
        "            recognized_mask = (pat_idx >= 0) & (fc <= self.max_forced_bits)\n",
        "            out_flat = torch.empty_like(flat_w, dtype=torch.int16, device=device)\n",
        "\n",
        "            w_off = (flat_w + 128).clamp(0,255).to(torch.long)\n",
        "            pat_idx_safe = pat_idx.clamp_min(0)\n",
        "            out_lut = self.global_lut[w_off, pat_idx_safe]\n",
        "            out_flat[recognized_mask] = out_lut[recognized_mask]\n",
        "\n",
        "            # fallback\n",
        "            bad_mask= ~recognized_mask\n",
        "            if bad_mask.any():\n",
        "                bad_idx = torch.where(bad_mask)[0]\n",
        "                bad_w   = flat_w[bad_idx]\n",
        "                bad_f   = faults_2d[:, bad_idx]\n",
        "                best_val_bad = fallback_many_weights(bad_w, bad_f, bitwidth)\n",
        "                out_flat[bad_idx] = best_val_bad\n",
        "\n",
        "            new_weight_2d = out_flat.view(in_feats, out_feats)\n",
        "\n",
        "        elif fault_mitigation == 'flip':\n",
        "            # row-block approach, sign-flip\n",
        "            row_start=0\n",
        "            while row_start < in_feats:\n",
        "                row_end = min(row_start+row_block_size, in_feats)\n",
        "                block_height = row_end - row_start\n",
        "\n",
        "                block_2d = weight_2d[row_start:row_end, :]\n",
        "                start_idx= row_start*out_feats\n",
        "                end_idx  = row_end*out_feats\n",
        "                block_faults = faults_2d[:, start_idx:end_idx]\n",
        "\n",
        "                block_flat = block_2d.view(-1).to(torch.int16)\n",
        "                fm, fv, fc = make_forced_mask_val(block_faults)\n",
        "                combo_id   = ((fm << 8) | fv).to(torch.int64)\n",
        "                pat_idx    = self.pattern_id_map[combo_id].to(torch.long)\n",
        "                recognized_mask = (pat_idx>=0) & (fc<=self.max_forced_bits)\n",
        "\n",
        "                pos_val = torch.empty_like(block_flat, dtype=torch.int16, device=device)\n",
        "                neg_val = torch.empty_like(block_flat, dtype=torch.int16, device=device)\n",
        "\n",
        "                # LUT for recognized\n",
        "                plus_flat  = block_flat\n",
        "                minus_flat = -block_flat\n",
        "                w_off_pos  = (plus_flat + 128).clamp(0,255).to(torch.long)\n",
        "                w_off_neg  = (minus_flat+128).clamp(0,255).to(torch.long)\n",
        "                pat_idx_safe = pat_idx.clamp_min(0)\n",
        "\n",
        "                pos_lut = self.global_lut[w_off_pos, pat_idx_safe]\n",
        "                neg_lut = self.global_lut[w_off_neg, pat_idx_safe]\n",
        "                pos_val[recognized_mask] = pos_lut[recognized_mask]\n",
        "                neg_val[recognized_mask] = neg_lut[recognized_mask]\n",
        "\n",
        "                # fallback for bad\n",
        "                bad_mask = ~recognized_mask\n",
        "                if bad_mask.any():\n",
        "                    bad_idx = torch.where(bad_mask)[0]\n",
        "                    b_w_pos = plus_flat[bad_idx]\n",
        "                    b_w_neg = minus_flat[bad_idx]\n",
        "                    b_f     = block_faults[:, bad_idx]\n",
        "                    best_p  = fallback_many_weights(b_w_pos, b_f, bitwidth)\n",
        "                    best_n  = fallback_many_weights(b_w_neg, b_f, bitwidth)\n",
        "                    pos_val[bad_idx] = best_p\n",
        "                    neg_val[bad_idx] = best_n\n",
        "\n",
        "                # reshape\n",
        "                block_out_pos = pos_val.view(block_height, out_feats)\n",
        "                block_out_neg = neg_val.view(block_height, out_feats)\n",
        "\n",
        "                # sum error column-wise in int32\n",
        "                diff_pos = (block_out_pos - block_2d).abs().to(torch.int32)\n",
        "                diff_neg = (block_out_neg + block_2d).abs().to(torch.int32)\n",
        "                sum_err_pos = diff_pos.sum(dim=0)\n",
        "                sum_err_neg = diff_neg.sum(dim=0)\n",
        "\n",
        "                final_block = torch.empty_like(block_out_pos, dtype=torch.int16, device=device)\n",
        "                better_mask = (sum_err_pos <= sum_err_neg)\n",
        "                final_block[:, better_mask]  = block_out_pos[:, better_mask]\n",
        "                # If neg is better, store the 'negative' of block_out_neg\n",
        "                # but remember block_out_neg is already \"the best read for -w\"\n",
        "                # to keep the final consistent with the sign flip, we must store -1 * that\n",
        "                final_block[:, ~better_mask] = (-block_out_neg[:, ~better_mask]).to(torch.int16)\n",
        "\n",
        "                new_weight_2d[row_start:row_end, :] = final_block\n",
        "                row_start=row_end\n",
        "\n",
        "        elif fault_mitigation == 'flip-bitwise':\n",
        "            # row-block approach, enumerates 256 bit flips in ONE PASS per block\n",
        "            num_flips = 1 << bitwidth  # 256\n",
        "\n",
        "            # Precompute \"flip_vectors\" in int8 or int16\n",
        "            flip_u = torch.arange(num_flips, dtype=torch.int16, device=device)  # [0..255]\n",
        "            bpos   = torch.arange(bitwidth,  dtype=torch.int16, device=device).view(1, -1)\n",
        "            flip_bits = ((flip_u.unsqueeze(-1) >> bpos) & 1).to(torch.int16)  # shape(256, bitwidth)\n",
        "            # Convert [0,1] bits to +1 or -1 flips:\n",
        "            flip_vectors = (1 - 2*flip_bits)  # shape(256, bitwidth), each in {+1, -1}\n",
        "\n",
        "            row_start=0\n",
        "            while row_start<in_feats:\n",
        "                row_end = min(row_start+row_block_size, in_feats)\n",
        "                block_height= row_end - row_start\n",
        "\n",
        "                block_2d     = weight_2d[row_start:row_end,:]     # (block_height, out_feats)\n",
        "                start_idx    = row_start*out_feats\n",
        "                end_idx      = row_end*out_feats\n",
        "                block_faults = faults_2d[:, start_idx:end_idx]     # (bitwidth, block_height*out_feats)\n",
        "\n",
        "                # Flatten block => shape(N,)\n",
        "                N = block_height*out_feats\n",
        "                block_flat_s16 = block_2d.view(-1).to(torch.int16)\n",
        "\n",
        "                # Build forced_mask/val for the base block faults\n",
        "                fm, fv, fc = make_forced_mask_val(block_faults)\n",
        "                recognized_mask = (fc <= self.max_forced_bits)\n",
        "\n",
        "                # Keep track of the best error (int32) and best read (int16) for each column\n",
        "                col_err = torch.full((out_feats,), 999999, dtype=torch.int32, device=device)\n",
        "                col_best= torch.empty((block_height, out_feats), dtype=torch.int16, device=device)\n",
        "\n",
        "                # We'll do a single pass through all 256 flips:\n",
        "                #   For each \"flip vector\", we multiply the block_faults by that flip pattern\n",
        "                #   Then see what the best read is (LUT or fallback).\n",
        "                #   Then compute the total error per column and keep track if it's better.\n",
        "\n",
        "                # Expand block_2d for column-wise error\n",
        "                block_2d_s16 = block_2d.to(torch.int16)\n",
        "\n",
        "                # Precompute repeated w => shape(1, N)\n",
        "                repeated_w = block_flat_s16.unsqueeze(0)\n",
        "\n",
        "                # We will try all 256 flips in one shot:\n",
        "                # shape(256, bitwidth, N)\n",
        "                # For each flip row: expanded_faults[i] = block_faults * flip_vectors[i]\n",
        "                flip_vectors_3d = flip_vectors.unsqueeze(-1)  # shape(256, bitwidth, 1)\n",
        "                # shape(1, bitwidth, N)\n",
        "                base_fault_3d   = block_faults.unsqueeze(0).to(torch.int16)\n",
        "                # Multiply => shape(256, bitwidth, N)\n",
        "                all_flips = base_fault_3d * flip_vectors_3d\n",
        "\n",
        "                # Now flatten so we can call make_forced_mask_val:\n",
        "                # shape(bitwidth, 256*N) => we want that, but we currently have (256, bitwidth, N)\n",
        "                # we'll reorder to (bitwidth, 256*N):\n",
        "                all_flips_reordered = all_flips.permute(1, 0, 2).reshape(bitwidth, -1)\n",
        "\n",
        "                # Forced mask/val\n",
        "                fm2, fv2, fc2 = make_forced_mask_val(all_flips_reordered)  # each => shape(256*N,)\n",
        "                # Partition them back into 256 groups of N\n",
        "                fm2_2d = fm2.view(num_flips, N)\n",
        "                fv2_2d = fv2.view(num_flips, N)\n",
        "                fc2_2d = fc2.view(num_flips, N)\n",
        "\n",
        "                # pattern lookup\n",
        "                combo_id2 = ((fm2_2d.to(torch.int32) << 8) | fv2_2d.to(torch.int32)).to(torch.int64)\n",
        "                pat_idx2  = self.pattern_id_map[combo_id2].to(torch.int32)\n",
        "                rec_mask2 = (pat_idx2 >= 0) & (fc2_2d <= self.max_forced_bits)\n",
        "\n",
        "                # We'll accumulate all reads in out_vals => shape(256,N)\n",
        "                out_vals = torch.empty((num_flips, N), dtype=torch.int16, device=device)\n",
        "\n",
        "                # For recognized combos => LUT\n",
        "                w_off = (repeated_w + 128).clamp(0,255).to(torch.int64)  # shape(1,N)\n",
        "                w_off_expand = w_off.expand(num_flips, -1)               # shape(256,N)\n",
        "                safe_idx2 = pat_idx2.clamp_min(0)                        # shape(256,N)\n",
        "                # Use gather from global_lut => shape(256,N)\n",
        "                # global_lut => (256, numPatterns), we have w_off_expand in [0..255], safe_idx2 in [0..numPatterns-1]\n",
        "                # We'll flatten so we can index 2D in a single gather. One approach is:\n",
        "                #   out_lut_flat = global_lut[w_off_expand.view(-1), safe_idx2.view(-1)]\n",
        "                # Then reshape => (256,N).\n",
        "                w_off_flat = w_off_expand.reshape(-1)\n",
        "                pat_idx_flat= safe_idx2.reshape(-1)\n",
        "                out_lut_flat= self.global_lut[w_off_flat, pat_idx_flat]  # shape(256*N,)\n",
        "                out_lut_2d  = out_lut_flat.view(num_flips, N)\n",
        "\n",
        "                # Write recognized combos\n",
        "                recognized_2d = rec_mask2\n",
        "                out_vals[recognized_2d] = out_lut_2d[recognized_2d]\n",
        "\n",
        "                # Fallback for \"bad\" combos\n",
        "                bad_mask_2d = ~recognized_2d\n",
        "                if bad_mask_2d.any():\n",
        "                    bad_idx = torch.where(bad_mask_2d)\n",
        "                    # Each bad_idx => (flip_index, item_index)\n",
        "                    # flatten these out:\n",
        "                    flat_linear = bad_idx[0]*N + bad_idx[1]\n",
        "                    b_w = repeated_w.view(-1)[bad_idx[1]]  # shape(#bad,) in int16\n",
        "                    # shape(bitwidth, #bad)\n",
        "                    # we gather from all_flips => (256, bitwidth, N)\n",
        "                    # For a given flip i and item j => all_flips[i, :, j]\n",
        "                    # We'll do a gather for each pair.\n",
        "                    # Easiest is to index all_flips_reordered again (which is (bitwidth,256*N)),\n",
        "                    # for these same 'flat_linear' but offset by item?\n",
        "                    # Actually we can do:\n",
        "                    b_fault = all_flips_reordered[:, flat_linear]  # shape(bitwidth, #bad)\n",
        "\n",
        "                    best_vals_b = fallback_many_weights(b_w, b_fault, bitwidth)\n",
        "                    # place them in out_vals\n",
        "                    out_vals.view(-1)[flat_linear] = best_vals_b\n",
        "\n",
        "                # Now we have out_vals => shape(256, N).  Reshape each row => (block_height, out_feats)\n",
        "                out_vals_3d = out_vals.view(num_flips, block_height, out_feats)\n",
        "\n",
        "                # Compute column-wise error => shape(256, out_feats)\n",
        "                # We'll do int32 differences\n",
        "                block_2d_expand = block_2d_s16.unsqueeze(0).expand(num_flips, -1, -1)  # shape(256, block_height,out_feats)\n",
        "                diff = (out_vals_3d - block_2d_expand).abs().to(torch.int32)          # shape(256, block_height, out_feats)\n",
        "                sum_err_cols = diff.sum(dim=1)  # shape(256, out_feats), int32\n",
        "\n",
        "                # For each column, pick the flip index that yields minimal sum_err\n",
        "                # We'll track min over the axis=0.\n",
        "                # sum_err_cols => shape(256, out_feats)\n",
        "                best_flip_idx = torch.argmin(sum_err_cols, dim=0)  # shape(out_feats,)\n",
        "\n",
        "                # Now gather the actual best read\n",
        "                # col_best => shape(block_height, out_feats)\n",
        "                # For each column c, we want out_vals_3d[ best_flip_idx[c], :, c ]\n",
        "                # We'll do a gather in a loop or do fancy indexing:\n",
        "                # fancy indexing approach: out_vals_3d[ best_flip_idx, range(out_feats), ... ] won't work directly\n",
        "                # because out_vals_3d is (256, block_height, out_feats).\n",
        "                # We can do:\n",
        "                #    for c in 0..out_feats:\n",
        "                #       col_best[:, c] = out_vals_3d[best_flip_idx[c], :, c]\n",
        "                for c in range(out_feats):\n",
        "                    col_best[:, c] = out_vals_3d[best_flip_idx[c], :, c]\n",
        "\n",
        "                new_weight_2d[row_start:row_end, :] = col_best\n",
        "                row_start = row_end\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown fault_mitigation={fault_mitigation}\")\n",
        "\n",
        "        # store final\n",
        "        final_w = new_weight_2d.transpose(0,1).to(self.weight.dtype)\n",
        "        self.weight.data = final_w * self.weight_scale\n",
        "        print(f\"Done inject_faults with mode={fault_mitigation}. row_block_size={row_block_size}.\")\n",
        "\n",
        "\n",
        "    def forward(self,input):\n",
        "        input,inp_scale = self.input_quantizer(input) #inp_scale dim = [batch_size,]\n",
        "        input = Round.apply(input/inp_scale) #dim = [batch_size,no_of_tokens,x]\n",
        "        if self.inference_done:\n",
        "          weight = self.weight\n",
        "          weight_scale = self.weight_scale\n",
        "        else:\n",
        "          weight,weight_scale = self.weight_quantizer(self.weight) #weight_scale dim = [output_channels,]\n",
        "        out = Round.apply(F.linear(input, Round.apply(weight/weight_scale),bias = None)) #dimensions = [batch_size,no_of_tokens,output_channels]\n",
        "        out = out*inp_scale*weight_scale.view(1,self.out_features)\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias.view(1,self.out_features)\n",
        "        return out"
      ],
      "metadata": {
        "id": "QoSAd3Zl1jbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model-Wide Utility Functions**\n",
        "\n",
        "These helper functions traverse the entire model architecture to manage the custom layers:\n",
        "\n",
        "`inject_faults_to_model`: Recursively finds all `custom_QuantizedLinear_new_baseline` instances and triggers their fault injection logic with the specified error rate and mitigation strategy.\n",
        "\n",
        "`model_to_inference`: Switches all quantized layers to inference mode, ensuring weights are quantized/frozen and scales are fixed before the validation phase begins."
      ],
      "metadata": {
        "id": "TJLRld_qaSkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inject_faults_to_model(model,fault_rate,fault_mitigation):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, custom_QuantizedLinear_new_baseline):\n",
        "            module.inject_faults(fault_rate,fault_mitigation)\n",
        "\n",
        "def model_to_inference(model):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, custom_QuantizedLinear_new_baseline):\n",
        "            module.to_inference()"
      ],
      "metadata": {
        "id": "h44ZEzlW1l9w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standard validation function**"
      ],
      "metadata": {
        "id": "j7QUXlhkawVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,test_loader,criterion):\n",
        " with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for data,target in test_loader:\n",
        "        data,target = data.to('cuda'),target.to('cuda')\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output,target).item()\n",
        "        pred = output.data.max(1,keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        #print(correct)\n",
        "\n",
        "    acc = 100. * correct/len(test_loader.dataset)\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "GD6vZ_1U1oO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation (CIFAR-100)**\n",
        "\n",
        "Defines the validation preprocessing pipeline: images are resized to 248x248, center-cropped to 224x224, and normalized (according to https://huggingface.co/edadaltocg/vit_base_patch16_224_in21k_ft_cifar100). It then downloads the CIFAR-100 test dataset and initializes a dataloader for batched evaluation."
      ],
      "metadata": {
        "id": "0_PpEQ0ubQi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((248, 248), interpolation=Image.BICUBIC),  # Resize using bicubic interpolation\n",
        "    transforms.CenterCrop(224),      # to get close to 90% crop pct\n",
        "    transforms.ToTensor(),                                      # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize with specified mean and std\n",
        "])\n",
        "\n",
        "\n",
        "testset = datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fQposeb1qM3",
        "outputId": "9119977b-4373-4e24-e898-1671d63ef830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 58.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automated Model Conversion**\n",
        "\n",
        "A utility function that recursively traverses the model architecture to replace standard `nn.Linear` layers with the custom quantized variant (quantized_cls). It handles the transfer of pre-trained weights and biases to the new layers.\n",
        "\n",
        "Selective Replacement: Includes a skip_name parameter (defaulting to 'head') to exclude specific layers (like the final classification head) from quantization, preserving their original precision."
      ],
      "metadata": {
        "id": "YTPDn1oxcP3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_linear_layers(module, quantized_cls, skip_name='head'):\n",
        "    \"\"\"\n",
        "    Recursively replace nn.Linear layers with quantized_cls layers,\n",
        "    except for the layer whose immediate parent name is skip_name.\n",
        "    \"\"\"\n",
        "    for name, child in list(module.named_children()):\n",
        "        # If the child is a Linear layer and its parent's name isn't skip_name, replace it\n",
        "        if isinstance(child, nn.Linear) and name != skip_name:\n",
        "            # Create a new quantized layer with the same dimensions\n",
        "            new_layer = quantized_cls(child.in_features, child.out_features, bias=(child.bias is not None))\n",
        "            # Copy over the weights and bias\n",
        "            with torch.no_grad():\n",
        "                new_layer.weight.copy_(child.weight)\n",
        "                if child.bias is not None:\n",
        "                    new_layer.bias.copy_(child.bias)\n",
        "            setattr(module, name, new_layer)\n",
        "        else:\n",
        "            # Recursively descend into children\n",
        "            replace_linear_layers(child, quantized_cls, skip_name=skip_name)"
      ],
      "metadata": {
        "id": "Quj_a-UJ1raX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment**\n",
        "\n",
        "\n",
        "Model Initialization: Loads the `vit_base_patch16_224` architecture and applies pre-trained CIFAR-100 weights from Hugging Face (https://huggingface.co/edadaltocg/vit_base_patch16_224_in21k_ft_cifar100). The classification head is manually resized to 100 classes to match the dataset.\n",
        "\n",
        "Quantization & Inference Setup: Converts all linear layers (excluding the head) to custom 8-bit layers and switches the model to inference mode (`model_to_inference`), which freezes the quantization scales.\n",
        "\n",
        "Fault Simulation Settings (`inject_faults_to_model`):\n",
        "\n",
        "Fault Rate: Simulating 1% to 5% rates with loop variable `j`.\n",
        "\n",
        "Mitigation Mode: Set to `none`, which uses the standard closest value mapping (CVM) approach. Other options include `flip` (Sign Flip) or `flip-bitwise` (Bit-Flip).\n",
        "\n",
        "Statistical Reliability: The inner loop (range(20)) repeats the evaluation 20 times. Since fault injection is stochastic (random bits are stuck each time), averaging the accuracy increases the statistical significance of the results."
      ],
      "metadata": {
        "id": "A3UMpfuEddwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(1,5):\n",
        "  avg_acc = 0\n",
        "  for i in range(20):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model = timm.create_model(\"timm/vit_base_patch16_224.orig_in21k_ft_in1k\",\n",
        "    pretrained=False)\n",
        "    model.head = nn.Linear(model.head.in_features, 100)\n",
        "    model.load_state_dict(\n",
        "    torch.hub.load_state_dict_from_url(\n",
        "          \"https://huggingface.co/edadaltocg/vit_base_patch16_224_in21k_ft_cifar100/resolve/main/pytorch_model.bin\",\n",
        "          map_location=\"cpu\",\n",
        "          file_name=\"vit_base_patch16_224_in21k_ft_cifar100.pth\",))\n",
        "    replace_linear_layers(model, custom_QuantizedLinear_new_baseline, skip_name='head')\n",
        "    model = model.to('cuda')\n",
        "    model_to_inference(model)\n",
        "    inject_faults_to_model(model,j,\"flip-bitwise\")\n",
        "    acc = test(model,testloader, criterion)\n",
        "    avg_acc += acc\n",
        "  print(avg_acc/20)"
      ],
      "metadata": {
        "id": "Io--FYNeTrkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}